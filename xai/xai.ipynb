{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUi1aMnq6G6v"
      },
      "source": [
        "# Practical session: Interpretability in Machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0wV1cm76Ec6"
      },
      "source": [
        "Machine learning algorithms often act as black boxes, with their decision-making processes being opaque. In this practical session, we aim to shed light on these processes, enhancing our understanding and interpretability of machine learning models. We will start by exploring model agnostic methods, which are techniques applicable to any machine learning model, regardless of its internal workings. Following this, we will delve into specific techniques tailored for neural network models.\n",
        "\n",
        "## Model agnostic Methods\n",
        "\n",
        "To begin, we will tackle a simple regression problem. This hands-on exercise will serve as an introduction to the tools and techniques used for model interpretation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCGRXcZMXZqA"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "We will begin with a simple regression problem of predicting Californian houses' house prices according to 8 numerical features.  \n",
        "Scikit-learn provides the dataset, and a full description is available [here](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SD1-P6XvA1yR"
      },
      "outputs": [],
      "source": [
        "!pip install ydata-profiling > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqNMicz1GBMV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "cal_housing = fetch_california_housing()\n",
        "feature_names = cal_housing.feature_names\n",
        "X = pd.DataFrame(cal_housing.data, columns=feature_names)\n",
        "y = cal_housing.target\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIxE8zIq9GIk"
      },
      "source": [
        "I encourage you to explore the dataset by yourself using pandas and seaborn; it is always a good exercise.  \n",
        "Nonetheless, today's practical session is not designed to train the best possible models but to learn how to interpret them.  \n",
        "It may be a good opportunity to present you a friendly tool for exploring datasets: [YData profiling](https://docs.profiling.ydata.ai/latest/).  \n",
        "YData profiling provides an automatic data exploration tool and html reports in a one-liner.  \n",
        "I would recommend usually doing it by yourself in other projects, but it may still be helpful for having a quick overview of what your dataset looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PXxeEaQNsOG"
      },
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "ProfileReport(X, sort=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g4BiV6a9Y_e"
      },
      "source": [
        "Now use scikit-learn's ```train_test_split``` method to split your dataset in train and test (10%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xEmnWncGDnN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhANGvb59x5b"
      },
      "source": [
        "Train a linear regression, a random forest and a neural network to predict the houses price.  \n",
        "Test all models on your test set to copare their performances.  \n",
        "Feel free to train and test more sophisticated models such as XGBoost, LightGBM..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mcrai3CT0vGy"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "lr = ...\n",
        "\n",
        "rf = ...\n",
        "\n",
        "\n",
        "mlp = ...\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "rf.fit(X_train, y_train)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "\n",
        " ... #test your models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPuKaOB56qb2"
      },
      "source": [
        "Linear models are considered intrinsically interpretable.  \n",
        "Using the ```coef_``` attribute of your model, visualize the importance of each of the features of the linear model.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtCSl7b25Vyk"
      },
      "outputs": [],
      "source": [
        "# to access the model part of your pipeline: lr[1]\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28qvhcHziwvy"
      },
      "source": [
        "### Features importance\n",
        "\n",
        "While Scikit-learn offers native methods for computing feature importance in tree-based models, we will explore a more versatile approach suitable for various model types.\n",
        "\n",
        "We will first utilize the feature permutation method, implemented in an external library called [Eli5](https://eli5.readthedocs.io/en/latest/overview.html#features).  \n",
        "Feature permutation is a model-agnostic technique that measures the importance of a feature by calculating the increase in the model's prediction error after permuting the feature's values. This method disrupts the relationship between the feature and the target, thereby highlighting the feature's influence on the model's predictive accuracy. By using feature permutation, we can evaluate the impact of each feature across our three different models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYX22dwpJ9s5"
      },
      "outputs": [],
      "source": [
        "!pip install eli5 > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwtyLPeXABT6"
      },
      "source": [
        "Use the ```PermutationImportance``` to compute the features importance of your models.  (Documentation [here](https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html)).  \n",
        "Plot them for each of your model.  \n",
        "Are the feature importance of the linear model similar to the coefficients?\n",
        "Are the features as important for all your models?  \n",
        "Create a dictionnary containing the top 5 features for each of your model (**key**:model name, **value**: dataframe of features importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYqFc4J4WveZ"
      },
      "outputs": [],
      "source": [
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "features_importance_dict = {}\n",
        "for model, name in zip([lr, rf, mlp], ['logistic regression', 'random forest', 'multi layer perceptron']):\n",
        "  plt.figure()\n",
        "  permumtation_import = PermutationImportance(...)\n",
        "  features_importance = {'Feature_name':feature_names, 'Importance':permumtation_import.feature_importances_}\n",
        "  features_importance = pd.DataFrame(features_importance) # dataframe containing the features names and their importance\n",
        "  features_importance = features_importance.sort_values(...) # sort the dataframe by feature importance\n",
        "  features_importance_dict[name] = ... #add the dataframe to your dictionnary\n",
        "  ax = sns.barplot(x=..., y=..., data=features_importance) #plot the model's features importance\n",
        "  plt.title(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSFXcCM_CCuM"
      },
      "source": [
        "If you've noticed that geographical location is an important feature in some of your models, you can use a tool called [folium](http://python-visualization.github.io/folium/) to visualize this. Folium makes it easy to plot these features on a map, giving us a clearer picture of how they affect house prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEuk1hj2XBv-"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "\n",
        "latmean = X.Latitude.mean()\n",
        "lonmean = X.Longitude.mean()\n",
        "map = folium.Map(location=[latmean,lonmean],\n",
        "        zoom_start=6)\n",
        "\n",
        "def color(value):\n",
        "    if value in range(0,149999):\n",
        "        col = 'green'\n",
        "    elif value in range(150000,249999):\n",
        "        col = 'yellow'\n",
        "    elif value in range(250000,349999):\n",
        "        col = 'orange'\n",
        "    else:\n",
        "        col='red'\n",
        "    return col\n",
        "\n",
        "map = folium.Map(location=[latmean,lonmean],\n",
        "        zoom_start=6)\n",
        "\n",
        "# Top three smart phone companies by market share in 2016\n",
        "for lat,lan,value in zip(X_test['Latitude'][:300],X_test['Longitude'][:300],y_test[:100]*100000):\n",
        "    folium.Marker(location=[lat,lan],icon= folium.Icon(color=color(value),icon_color='black',icon = 'home')).add_to(map)\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsCwO3iJCJr8"
      },
      "source": [
        "### PDP and ICE Plots\n",
        "\n",
        "Partial Dependence Plots (PDP) and Individual Conditional Expectation (ICE) plots are powerful tools for interpreting machine learning models. They help us understand the relationship between the target response and a set of input features of interest.\n",
        "\n",
        "For generating these plots, we will use the [pdpbox](https://pdpbox.readthedocs.io/en/latest/) library. PDP shows the average effect of a feature on the prediction, while ICE plots display this effect for individual instances, providing a more detailed view.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mo1wzI6fE25z"
      },
      "outputs": [],
      "source": [
        "!pip install pdpbox > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWXTqLfiCfhx"
      },
      "source": [
        "The following code shows you how to produce a PDP plot for the random forest model.  \n",
        "```python\n",
        "from pdpbox import pdp, get_dataset, info_plots\n",
        "\n",
        "pdp_feat = pdp.PDPIsolate(model=rf, dataset=X_test.copy(), # need to copy() since PDPIsolate modifies the df\n",
        "                            model_features=feature_names, feature='MedInc',\n",
        "                            n_classes=0, num_grid_points=50)\n",
        "pdp_feat.plot(engine='matplotlib', plot_lines=True)\n",
        "```\n",
        "Use it to generate the PDP plots for the three most important features of each of your models.  \n",
        "What is the nature of their relationship with the target?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHGF1svD8tQH"
      },
      "outputs": [],
      "source": [
        "from pdpbox import pdp\n",
        "model = rf #lr, mlp\n",
        "model_name = 'random forest'#'logistic regression' , 'multi layer perceptron'\n",
        "\n",
        "top_3_features = features_importance_dict[model_name].Feature_name[:3].values\n",
        "for i, feature in enumerate(top_3_features, 1):\n",
        "  pdp_dist = ...\n",
        "  ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui2NTJPuDWEj"
      },
      "source": [
        "It is also possible to visualize the combined effetc of two features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GROohofH8tKw"
      },
      "outputs": [],
      "source": [
        "interact = pdp.PDPInteract(model=model, df=X_test.copy(), model_features=feature_names,\n",
        "                             features=['Latitude', 'Longitude'],\n",
        "                             feature_names=['Latitude', 'Longitude'],\n",
        "                             n_classes=0)\n",
        "interact.plot(engine='matplotlib', plot_type='contour')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ix2ghOXD9fE"
      },
      "source": [
        "Scikit-learns also provides methods to generate such plots, but may offer less flexibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oClY7KYiZfLa"
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import partial_dependence\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "\n",
        "for model, model_name in zip([lr, rf, mlp], ['logistic regression', 'random forest', 'multi layer perceptron']):\n",
        "\n",
        "  top_3_features = features_importance_dict[name].Feature_name[:3].values\n",
        "  display = PartialDependenceDisplay.from_estimator(\n",
        "    model,\n",
        "    X_test,\n",
        "    top_3_features,\n",
        "    kind=\"both\",\n",
        "    subsample=50,\n",
        "    n_jobs=3,\n",
        "    n_cols=3,\n",
        "    grid_resolution=20,\n",
        "    random_state=0,\n",
        "    ice_lines_kw={\"color\": \"tab:blue\", \"alpha\": 0.2, \"linewidth\": 0.5},\n",
        "    pd_line_kw={\"color\": \"tab:orange\", \"linestyle\": \"--\"}\n",
        "    )\n",
        "  display.figure_.suptitle(f\"Partial dependence for {model_name} model\")\n",
        "  display.figure_.subplots_adjust(hspace=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7B_FP1MLaC1"
      },
      "outputs": [],
      "source": [
        "for model, name in zip([lr, rf, mlp], ['logistic regression', 'random forest', 'multi layer perceptron']):\n",
        "  _, ax = plt.subplots(ncols=3, figsize=(9, 4))\n",
        "  top_2_features = features_importance_dict[name].Feature_name[:3].values\n",
        "  features = [top_2_features[0], top_2_features[1], (top_2_features[0], top_2_features[1])]\n",
        "  display = PartialDependenceDisplay.from_estimator(\n",
        "      model,\n",
        "      X_test,\n",
        "      features,\n",
        "      kind=\"average\",\n",
        "      n_jobs=3,\n",
        "      grid_resolution=20,\n",
        "      ax=ax,\n",
        "  )\n",
        "  display.figure_.suptitle(f\"Partial dependence for {name} model\")\n",
        "  display.figure_.subplots_adjust(wspace=0.4, hspace=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qcHjlNlqZht"
      },
      "source": [
        "### SHAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqDHTKMk89DV"
      },
      "source": [
        "Previous methods provided global explanations of our models.  \n",
        "We will now focus on local interpretability methods.  \n",
        "We will begin with the SHAP methods based on the estimation of the Shapley values.  \n",
        "The library [SHAP](https://github.com/shap/shap) implements the SHAP method (and many others).\n",
        "\n",
        "Inspire yourself with the following [examples](https://github.com/shap/shap) to produce a visualization of the estimated Shapley values of your different models, first for a single example using the ```force``` method and for the entire test, dataset using the ```summary_plot``` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL3bFS9fJ74P"
      },
      "outputs": [],
      "source": [
        "!pip install shap > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anuzREMVPrtw"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import shap\n",
        "shap.initjs() #needed to plot results directly on the notebook\n",
        "\n",
        "idx = 1 # index of the instance we want to explain\n",
        "\n",
        "explainer = ...\n",
        "shap_values = explainer(X_test.iloc[0:20,:]) #To speed up we just compute the shap values for 100 exemples\n",
        "... # use option , matplotlib=matplotlib to display in colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmXkCsJrBv2n"
      },
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_values, X_test.iloc[0:20,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGiokmqJXJOA"
      },
      "source": [
        "### LIME (Local Interpretable Model-agnostic Explanations)\n",
        "\n",
        "LIME is an innovative technique for explaining the predictions of any machine learning classifier in an understandable way. It helps in understanding why a model makes certain predictions, focusing on individual instances.\n",
        "\n",
        "During our class, we discussed LIME as a model agnostic local interpretability method. This approach is valuable for gaining insights into specific predictions, especially in complex models. For practical application, we will use the [LIME implementation provided by the original authors](https://github.com/marcotcr/lime), which is well-documented and widely used in the Python community.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gqZDHzaNQmJ"
      },
      "outputs": [],
      "source": [
        "!pip install lime > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNJcxPiWHxQZ"
      },
      "source": [
        "LIME provides eay to understand an friendly looking explanations for your model predictions.  \n",
        "You first need to instanciate an Explainer (in our case a ```LimeTabularExplainer```) and then call the ```explain instance``` method of the explainer to get the explanations.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxD3boqfDuOL"
      },
      "outputs": [],
      "source": [
        "import lime\n",
        "import lime.lime_tabular\n",
        "\n",
        "index = 0\n",
        "\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(X_test.values, feature_names=feature_names, mode=\"regression\")\n",
        "exp = explainer.explain_instance(X_test.iloc[index], rf.predict, num_features=5, top_labels=1)\n",
        "exp.show_in_notebook(show_table=True, show_all=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhF1RRIvFr9W"
      },
      "source": [
        "#### Classification\n",
        "LIME also works with classification problems.  \n",
        "We will repeat the previous experiment using a different dataset for [breast cancer prediction](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset) and a decision trees algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO_5AJ48iMr0"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "breast_cancer = load_breast_cancer()\n",
        "feature_names = breast_cancer.feature_names\n",
        "target_names = breast_cancer.target_names\n",
        "X = pd.DataFrame(breast_cancer.data, columns=feature_names)\n",
        "y = breast_cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQZIu3brGdp8"
      },
      "source": [
        "Train a decision tree (with max_depth=5) on this dataset and plot the confusion matrix on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pM4Q_niCIoYG"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "dt = ...\n",
        "...\n",
        "print(f\"Descision Tree score: {dt.score(X_test, y_test):.2f}\")\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYfBIwhWJUj7"
      },
      "source": [
        "Decision trees are also interpretable models.  \n",
        "Scikit-learn provides an efficient way to visualize their structure.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XliLIQjTJGYe"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sklearn.tree as tree\n",
        "plt.figure(figsize=(20,20))\n",
        "tree.plot_tree(dt[1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6G-XEoAG3a5"
      },
      "source": [
        "Explain the predictions of your model on some examples.  \n",
        "For classification tasks, LIME needs the predicted \"probailities\" of the model.  \n",
        "Use the ```predict_proba``` method of your classifier instead of the ```predict``` method when calling the explain instance.    \n",
        "Also, don't forget to remove the ```mode=\"regression\"``` argument when instanciating the ```LimeTabularExplainer```.  \n",
        "Are the explanations consistent with the decision graph?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4phdYE0it1h"
      },
      "outputs": [],
      "source": [
        "explainer = lime.lime_tabular.LimeTabularExplainer(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08wSF1wNldmz"
      },
      "outputs": [],
      "source": [
        "index = 0\n",
        "exp = explainer.explain_instance(...)\n",
        "exp.show_in_notebook(show_table=True, show_all=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJQ6Y29fiE-8"
      },
      "source": [
        "#### Text data\n",
        "The authors also provided nice visualizations for text data.  \n",
        "We will now train a Random forest to classify whether scientific texts are about medicine or space.   \n",
        "We will be using two categories from the [newsgroup dataset](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) available in scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jb1uEpxVWNaw"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = [\n",
        "    'sci.med',\n",
        "    'sci.space'\n",
        "]\n",
        "\n",
        "\n",
        "train_data = fetch_20newsgroups(subset='train', categories=categories)\n",
        "test_data = fetch_20newsgroups(subset='test', categories=categories)\n",
        "\n",
        "class_names = train_data.target_names\n",
        "\n",
        "X_train, y_train  = train_data.data, train_data.target\n",
        "X_test, y_test = test_data.data, test_data.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7TcROIKPr-q"
      },
      "source": [
        "Here is an example of text to classify:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fy5fe70LPQxU"
      },
      "outputs": [],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF3PCkDNPza6"
      },
      "source": [
        "Let's train a random forest to classify our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcuQRF56Pzva"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(\n",
        "        CountVectorizer(max_df= 0.5, ngram_range= (1, 2)),\n",
        "        TfidfTransformer(),\n",
        "        RandomForestClassifier()\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "print(f\"Model score: {model.score(X_test, y_test):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDpK2DSpO67P"
      },
      "source": [
        "We will use a specific ```LimeTextExplainer``` to explain the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jShOjASkaKqW"
      },
      "outputs": [],
      "source": [
        "from lime.lime_text import LimeTextExplainer\n",
        "explainer = LimeTextExplainer(class_names=class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OioyQTXsbbJC"
      },
      "outputs": [],
      "source": [
        "from lime.lime_text import LimeTextExplainer\n",
        "explainer = LimeTextExplainer(class_names=class_names)\n",
        "\n",
        "index = 11\n",
        "exp = explainer.explain_instance(X_test[index], model.predict_proba, num_features=6)\n",
        "\n",
        "prediction = model.predict_proba([X_test[index]])\n",
        "class_predicted = class_names[prediction.argmax(1)[0]]\n",
        "class_proba = prediction.max(1)[0]\n",
        "true_class = class_names[y_test[index]]\n",
        "print(f'Class predicted: {class_predicted} (p={class_proba})')\n",
        "print(f'True class: {class_names[y_test[index]]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-CnzTmiQP5H"
      },
      "source": [
        "Here are the top words used by the classifier.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fBm-MrxewAr"
      },
      "outputs": [],
      "source": [
        "fig = exp.as_pyplot_figure()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5Qa3_eMQu3z"
      },
      "source": [
        "These explanations seem plausible, let's visualize these words in their context:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chdysnzoe4mz"
      },
      "outputs": [],
      "source": [
        "exp.show_in_notebook(text=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJgAQVl2Q1QA"
      },
      "source": [
        "Some of the words are in the newsgroup header!  \n",
        "Would you trust such a classifier?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7erm7a2RFoO"
      },
      "source": [
        "Scikit-learn provides an option to remove all headers and footers.\n",
        "Train a new model on the datset with removed headers and footers and comare its F1-score with the previous model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amTysYs5fKUr"
      },
      "outputs": [],
      "source": [
        "train_data = fetch_20newsgroups(subset='train',remove=('headers', 'footers', 'quotes'),\n",
        "                                     categories=categories)\n",
        "\n",
        "\n",
        "class_names = train_data.target_names\n",
        "\n",
        "X_train, y_train  = train_data.data, train_data.target\n",
        "X_test, y_test = test_data.data, test_data.target\n",
        "\n",
        "\n",
        "model = ...\n",
        "...\n",
        "model.fit(X_train, y_train)\n",
        "print(f\"Model score: {model.score(X_test, y_test):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7__EA0CjRiSh"
      },
      "source": [
        "Now visualize the explainations computed by lime on the same example with your new model.  \n",
        "Which of the two models would you trust the more?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygTES-j-gNxM"
      },
      "outputs": [],
      "source": [
        "explainer = ...\n",
        "exp = ...\n",
        "\n",
        "prediction = ...\n",
        "class_predicted = ...\n",
        "class_proba = ...\n",
        "true_class = ...\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-nwQYZxplnx"
      },
      "source": [
        "#### Image Data\n",
        "\n",
        "Finally, LIME also provides friendly-looking visualizations on images.  \n",
        "We will use a subset of Imagenet to test these visualizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysRk_M-A9Qds"
      },
      "outputs": [],
      "source": [
        "!wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz > /dev/null 2>&1\n",
        "!tar zxvf imagenette2.tgz > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lthQ-VxA_su7"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "means, stds = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
        "train_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(means, stds),\n",
        "    ])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(means, stds),\n",
        "    ])\n",
        "\n",
        "def get_imagenette2_loaders(root_path='./imagenette2', **kwargs):\n",
        "\n",
        "    trainset = torchvision.datasets.ImageFolder(os.path.join(root_path, \"train\"), transform=train_transform)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, **kwargs)\n",
        "    testset = torchvision.datasets.ImageFolder(os.path.join(root_path, \"val\"), transform=test_transform)\n",
        "    testloader = torch.utils.data.DataLoader(testset, **kwargs)\n",
        "    return trainloader, testloader\n",
        "\n",
        "trainloader, testloader = get_imagenette2_loaders( batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "labels = ['tench', 'English springer', 'cassette player', 'chain saw', 'church', 'French horn', 'garbage truck', 'gas pump', 'golf ball', 'parachute']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmgiKjbPI7jS"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "inv_normalize = transforms.Normalize(\n",
        "   mean= [-m/s for m, s in zip(means, stds)],\n",
        "   std= [1/s for s in stds]\n",
        ")\n",
        "\n",
        "x, _ = next(iter(trainloader))\n",
        "img_grid = make_grid(x[:16])\n",
        "img_grid = inv_normalize(img_grid)\n",
        "plt.figure(figsize=(20,15))\n",
        "plt.imshow(img_grid.permute(1, 2, 0))\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zhmmKQgXEtv"
      },
      "source": [
        "We will train a neural network to classify these images.  \n",
        "However, training neural networks on high-definition images may be long and difficult.  \n",
        "We will use a pre-trained VGG11 and replace the fully connected part of the network to match the ten classes (this is called transfer learning).  \n",
        "Complete the following code to instantiate a model predicting among ten classes with pre-trained features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8wgIMYcBJP4"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "model = torchvision.models.vgg11(pretrained=True)\n",
        "for param in model.features:\n",
        "    param.requires_grad = False\n",
        "\n",
        "model.classifier = nn.Sequential(\n",
        "            ...\n",
        "        )\n",
        "\n",
        "model = model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geTOAMq2XIKC"
      },
      "source": [
        "Fill the following code to implement the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIpkmp0WCuzR"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "criterion_classifier = nn.CrossEntropyLoss(reduction='mean')\n",
        "\n",
        "def train(model, optimizer, trainloader, epochs=30):\n",
        "    t = tqdm(range(epochs))\n",
        "    for epoch in t:\n",
        "        corrects = 0\n",
        "        total = 0\n",
        "        for x, y in trainloader:\n",
        "            loss = 0\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "            y_hat = ...\n",
        "\n",
        "            loss += criterion_classifier(...)\n",
        "            _, predicted = y_hat.max(1)\n",
        "            corrects += predicted.eq(y).sum().item()\n",
        "            total += y.size(0)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            t.set_description(f'epoch:{epoch} current accuracy:{round(corrects / total * 100, 2)}%')\n",
        "    return (corrects / total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzs1TAmVXXwB"
      },
      "source": [
        "Train your model. One or two epochs should be enough since we are using transfer learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYtyT44D7-Go"
      },
      "outputs": [],
      "source": [
        "learning_rate = 5e-3\n",
        "epochs = 1\n",
        "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=learning_rate)\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOV8mtfYXoGM"
      },
      "source": [
        "Test your network to validate it has enough classification abilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Hsn0o04zF3n"
      },
      "outputs": [],
      "source": [
        "def test(model, dataloader):\n",
        "    test_corrects = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "            y_hat = model(x).argmax(1)\n",
        "            test_corrects += y_hat.eq(y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return test_corrects / total\n",
        "\n",
        "model.eval()\n",
        "test_acc = test(model, testloader)\n",
        "print(f'Test accuracy: {test_acc:.2f} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba3sRtcSYMbf"
      },
      "source": [
        "Using a single example, we will now use lime to visualize the important parts of the image for our model prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJm063ZpHrUh"
      },
      "outputs": [],
      "source": [
        "idx = 0\n",
        "\n",
        "img = inv_normalize(x[idx])\n",
        "np_img = np.transpose(img.cpu().detach().numpy(), (1,2,0))*255\n",
        "np_img = np_img.astype(np.uint8)\n",
        "plt.imshow(np_img)\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEkocJ9kYi72"
      },
      "source": [
        "Let's first verify our model prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gPOMxM3xIs-"
      },
      "outputs": [],
      "source": [
        "input = x[idx].unsqueeze(0).cuda()\n",
        "output = model(input)\n",
        "_, prediction = torch.topk(output, 1)\n",
        "print(f\"Model's prediction: {labels[prediction.item()]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mnNQ7jlZ0YD"
      },
      "source": [
        "LIME provide a ```LimeImageExplainer``` to deal with images.\n",
        "However, the  ```LimeImageExplainer``` requires a callable function that will directly produce predictions for a list of images (the perturbed from the original images) in the form of a numpy array.  \n",
        "Our pytorch model works with pytorch ```Tensor``` mini-batches and outputs ```Tensor``` objects.  \n",
        "We thus need to wrap our model and the associated pre-processing into a single callable function to use the ```LimeImageExplainer```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbSTIkPyDM5p"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "lime_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(means, stds),\n",
        "    ])\n",
        "\n",
        "def batch_predict(images):\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    batch = torch.stack(tuple(lime_transform(i) for i in images), dim=0)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    batch = batch.to(device)\n",
        "    logits = model(batch)\n",
        "  return logits.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BviJvjTWZ4Lf"
      },
      "source": [
        "We now can use the Lime explainer to visualize which parts of the images were the most important."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bi525ZiGE-JW"
      },
      "outputs": [],
      "source": [
        "from lime import lime_image\n",
        "from skimage.segmentation import mark_boundaries\n",
        "\n",
        "explainer = lime_image.LimeImageExplainer()\n",
        "explanation = explainer.explain_instance(np_img,\n",
        "                                         batch_predict, # classification function\n",
        "                                         top_labels=5,\n",
        "                                         hide_color=0,\n",
        "                                         num_samples=1000)\n",
        "\n",
        "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=10, hide_rest=False)\n",
        "img_boundry = mark_boundaries(temp/255.0, mask)\n",
        "plt.imshow(img_boundry)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqMXh4lZaK4e"
      },
      "source": [
        "## Model specific methods\n",
        "We just saw many methods for model agnostic interpretability.  \n",
        "The second part of this session is dedicated to model-specific methods.  \n",
        "In particular, we will implement methods specific to neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7qRGlB_u85-"
      },
      "source": [
        "### Vanilla gradient back-propagation\n",
        "\n",
        "We will now implement three methods to generate saliency maps on our images.  \n",
        "One of the simplest ways to generate saliency maps is certainly to backpropagate the gradients of the predicted output directly to the image input.  This method, called vanilla gradient backpropagation, is presented in this [article](Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps).\n",
        "\n",
        "Let's first visualize an image for which we will generate a saliency map according to our model's prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLMiENqtu--D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "x, _ = next(iter(trainloader))\n",
        "idx = 0\n",
        "\n",
        "img = x[idx]\n",
        "np_img = np.transpose(inv_normalize(img).cpu().detach().numpy(), (1,2,0))\n",
        "plt.imshow(np_img)\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LinUlN7v4RR"
      },
      "source": [
        "By default, input tensors do not require to generate gradients in Pytorch.  \n",
        "Thus, we first need to set the image to catch the gradient during the backward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vri3nLsAvUJf"
      },
      "outputs": [],
      "source": [
        "img = img.unsqueeze(0).cuda() # we need to set the input on GPU before the requires_grad operation!\n",
        "img.requires_grad_();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEMUhj3hwhWX"
      },
      "source": [
        "We will now compute the model's prediction for this image and backpropagate from this prediction to the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3sok1YRvn-q"
      },
      "outputs": [],
      "source": [
        "output = model(img)\n",
        "output_idx = output.argmax()\n",
        "output_max = output[0, output_idx]\n",
        "\n",
        "output_max.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkUxlktjw9Xs"
      },
      "source": [
        "We can now generate a saliency map were important pixels will correspond to important gradients!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eflE0Kv6vymQ"
      },
      "outputs": [],
      "source": [
        "saliency, _ = torch.max(img.grad.data.abs(), dim=1)\n",
        "saliency = saliency.squeeze(0)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(np_img)\n",
        "plt.axis('off')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(saliency.cpu(), cmap='hot')\n",
        "plt.axis('off')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGdX8kjIxX6g"
      },
      "source": [
        "Try with different images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDKNKK9wfTKF"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ciHu8Mq8KtO"
      },
      "source": [
        "### Smooth grad\n",
        "A simple way to generate smoother visualizations called [Smooth-grad](https://arxiv.org/pdf/1706.03825.pdf) consists of averaging saliency maps from augmented versions of the original image.  \n",
        "Complete the following function to generate the gradient of an image according to the model's prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_VTS4MJ1BlL"
      },
      "outputs": [],
      "source": [
        "def get_vanilla_grad(img, model):\n",
        "  ...#retain gradients\n",
        "  ...\n",
        "\n",
        "  return img.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhyNzQs8y1yG"
      },
      "source": [
        "We will now generate perturbated versions of the image by adding a gaussian noise to the original image.  \n",
        "For every image generated we will compute the corresponding gardients and average them to generate the final saliency map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEwQ7GmQ1jyi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "stdev_spread=0.15\n",
        "n_samples=100\n",
        "stdev = stdev_spread * (img.max() - img.min())\n",
        "total_gradients = torch.zeros_like(img, device='cuda')\n",
        "\n",
        "for i in range(n_samples):\n",
        "    noise = np.random.normal(0, stdev.item(), img.shape).astype(np.float32)\n",
        "    noisy_img = img + torch.tensor(noise, device='cuda', requires_grad=True)\n",
        "    grad= get_vanilla_grad(noisy_img, model)\n",
        "    total_gradients += grad * grad #using the square of the gradients generates smoother visualizations\n",
        "    #total_gradients += grad\n",
        "total_gradients /= n_samples\n",
        "\n",
        "saliency, _ = torch.max(total_gradients.abs(), dim=1)\n",
        "saliency = saliency.squeeze(0)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(np_img)\n",
        "plt.axis('off')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(saliency.cpu(), cmap='hot')\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ0nTGsBzp-Y"
      },
      "source": [
        "Try it with other images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A91PMCIMfTKG"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMW4AGjJARWJ"
      },
      "source": [
        "### Grad-CAM\n",
        "Instead of propagating the gradients to the inputs, the [Grad-CAM](https://arxiv.org/abs/1610.02391) method generates a saliency map by multiplying the outputs of the final feature map with an average of its gradients.  It thus generates coarse saliency maps, sometimes more relevant than pixels.  \n",
        "We will create a 'hook' to keep both the activations and the gardients of a network layer.  \n",
        "This operation is a bit tricky, just keep it mind that it is a way to keep activations and gradients in a single object during the forward and the backward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sHWPVIoAUDk"
      },
      "outputs": [],
      "source": [
        "class HookFeatures():\n",
        "    def __init__(self, module):\n",
        "        self.feature_hook = module.register_forward_hook(self.feature_hook_fn)\n",
        "    def feature_hook_fn(self, module, input, output):\n",
        "        self.features = output.clone().detach()\n",
        "        self.gradient_hook = output.register_hook(self.gradient_hook_fn)\n",
        "    def gradient_hook_fn(self, grad):\n",
        "        self.gradients = grad\n",
        "    def close(self):\n",
        "        self.feature_hook.remove()\n",
        "        self.gradient_hook.remove()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wJA1Ppa2Sf9"
      },
      "source": [
        "We will 'hook' the activations and gradients of the last convolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-g3YLoE2PVi"
      },
      "outputs": [],
      "source": [
        "print(model)\n",
        "hook = HookFeatures(model.features[19])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmeLAfyk2hP-"
      },
      "source": [
        "Similar to what we did before, we will backpropagate the gradients of the predicted output on the feature map this time and get both the activations and the gradients thanks to our hook on the last convolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDTxSM9L6Xd6"
      },
      "outputs": [],
      "source": [
        "output = model(img)\n",
        "output_idx = output.argmax()\n",
        "output_max = output[0, output_idx]\n",
        "output_max.backward()\n",
        "\n",
        "gradients = hook.gradients\n",
        "activations = hook.features\n",
        "pooled_gradients = torch.mean(gradients, dim=[0, 2, 3]) # we take the average gradient of every chanels\n",
        "for i in range(activations.shape[1]):\n",
        "    activations[:, i, :, :] *= pooled_gradients[i] # we multiply every chanels of the feature map with their corresponding averaged gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RHfl02a3q0m"
      },
      "source": [
        "We can now take the average of all channels of the gradient weighted feature map to generate a heat map, keeping only the positive values to get the positive influences only.  \n",
        "We also need to reshape the generated heat map to math the original input size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GLSW8qVHR2q"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "heatmap = torch.mean(activations, dim=1).squeeze()\n",
        "heatmap = np.maximum(heatmap.detach().cpu(), 0)\n",
        "heatmap /= torch.max(heatmap)\n",
        "heatmap = cv2.resize(np.float32(heatmap), (img.shape[2], img.shape[3]))\n",
        "heatmap = np.uint8(255 * heatmap)\n",
        "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_RAINBOW) / 255\n",
        "superposed_img = (heatmap) * 0.4 + np_img\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(np.clip(superposed_img,0,1))\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayXfEGQy4P4Q"
      },
      "source": [
        "We must remove our hook, it won't be used in the rest of the session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LV9IkpD7nOyl"
      },
      "outputs": [],
      "source": [
        "hook.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko1tBuUhE8aB"
      },
      "source": [
        "### Captum  \n",
        "\n",
        "[Captum](https://captum.ai/)  is a library developed by Facebook to generate explanations on Pytorch models.  \n",
        "It implements various other saliency maps methods that we did not cover during our class.\n",
        "You should look at the doc and find out [other possible methods](https://captum.ai/docs/algorithms) to gain interpretability in your Pytorch models.  \n",
        "We will quickly try some of them so you know how to use them if you need to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkCYNgf-J6qN"
      },
      "outputs": [],
      "source": [
        "!pip install captum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12e56myGJ3kQ"
      },
      "outputs": [],
      "source": [
        "from captum.attr import IntegratedGradients\n",
        "from captum.attr import GradientShap\n",
        "from captum.attr import Occlusion\n",
        "from captum.attr import NoiseTunnel\n",
        "from captum.attr import GuidedGradCam\n",
        "from captum.attr import DeepLift\n",
        "from captum.attr import visualization as viz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw7EzoAq51sN"
      },
      "outputs": [],
      "source": [
        "def plot_heatmap(attributions, img):\n",
        "  _ = viz.visualize_image_attr_multiple(np.transpose(attributions.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                              np.transpose(inv_normalize(img).squeeze().cpu().detach().numpy(), (1,2,0)),\n",
        "                              methods=[\"original_image\", \"heat_map\"],\n",
        "                              signs=['all', 'positive'],\n",
        "                              cmap='hot',\n",
        "                              show_colorbar=True)\n",
        "\n",
        "# Integradted gradients (https://arxiv.org/abs/1703.01365)\n",
        "integrated_gradients = IntegratedGradients(model)\n",
        "attributions = integrated_gradients.attribute(img, target=output_idx, n_steps=200, internal_batch_size=1)\n",
        "\n",
        "plot_heatmap(attributions, img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9PussT6q3Gg"
      },
      "outputs": [],
      "source": [
        "#Noise tunnel (SmoothGrad, VarGrad: https://arxiv.org/abs/1810.03307)\n",
        "noise_tunnel = NoiseTunnel(integrated_gradients)\n",
        "attributions = noise_tunnel.attribute(img, nt_samples_batch_size=1, nt_samples=10, nt_type='smoothgrad_sq', target=output_idx)\n",
        "\n",
        "plot_heatmap(attributions, img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdBAjCeySQcz"
      },
      "outputs": [],
      "source": [
        "#Occlusion (https://arxiv.org/abs/1311.2901)\n",
        "occlusion = Occlusion(model)\n",
        "attributions = occlusion.attribute(img,\n",
        "                                  strides = (3, 8, 8),\n",
        "                                  target=output_idx,\n",
        "                                  sliding_window_shapes=(3,15, 15),\n",
        "                                  baselines=0)\n",
        "\n",
        "plot_heatmap(attributions, img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YW34crn9Vo5i"
      },
      "outputs": [],
      "source": [
        "#DeepLift (https://arxiv.org/pdf/1704.02685.pdf)\n",
        "dl = DeepLift(model)\n",
        "attributions = dl.attribute(img, target=output_idx, baselines=img * 0)\n",
        "\n",
        "plot_heatmap(attributions, img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3_D4BBbWz3z"
      },
      "outputs": [],
      "source": [
        "#Guided Grad-CAM (https://arxiv.org/abs/1610.02391)\n",
        "guided_gc = GuidedGradCam(model, model.features[19])\n",
        "attribution = guided_gc.attribute(img, target=output_idx)\n",
        "plot_heatmap(attributions, img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzdhApmtnwlh"
      },
      "source": [
        "## Feature visualization\n",
        "Previous methods, generating saliency maps were model-specific local-interpretability methods.  \n",
        "We will now implement a global interpretability called feature visualization.  \n",
        "I really encourage you to read the [distill publication](https://distill.pub/2017/feature-visualization/) presenting the methods.  The obtained visualizations are superb!\n",
        "The principle of the method is straightforward. It consists of optimizing a random image to maximize the output of a neural network unit.  \n",
        "A unit can be a neuron, a channel of a feature map, or an entire feature map.  \n",
        "In this practical session, we will focus on channels of a pre-trained VGG19 network.\n",
        "Let's first have a look at our network architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qIwyf2IPL38"
      },
      "outputs": [],
      "source": [
        "model = torchvision.models.vgg19(pretrained=True).cuda()\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WuhEYp4Kn7P"
      },
      "source": [
        "Once again, we will create a hook to keep the activation of intermediate layers.  \n",
        "These activations will be used to generate the feature visualizations and serve as a loss function to optimize the random image.  \n",
        "We won't need to keep the gradients this time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDthf9CzKloB"
      },
      "outputs": [],
      "source": [
        "class FeaturesHook():\n",
        "    def __init__(self, module):\n",
        "        self.hook = module.register_forward_hook(self.hook_fn)\n",
        "    def hook_fn(self, module, input, output):\n",
        "        self.features = output\n",
        "    def close(self):\n",
        "        self.hook.remove()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrzkFp14Mh_S"
      },
      "source": [
        "The following code will compute the feature visualization for one channel of one layer.  \n",
        "We begin by initializing a random image and creating a hook on the desired layer.  \n",
        "Then we will do a forward pass of our image through the network and compute a loss to maximize the hooked layer's desired channel.  \n",
        "We will repeat this operation several epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOCsazzOK6CX"
      },
      "outputs": [],
      "source": [
        "def visualize_feature(model, layer_idx, channel_idx):\n",
        "  img = torch.rand((1,3,224,224), requires_grad=True, device=\"cuda\") #initialize a random image\n",
        "  optimizer = torch.optim.Adam([img], lr=0.1, weight_decay=1e-6)\n",
        "  features_hook =  FeaturesHook(model.features[layer_idx]) # hook the desired layer\n",
        "  for n in range(512):\n",
        "        optimizer.zero_grad()\n",
        "        model(img) #forward pass\n",
        "        features_map = features_hook.features\n",
        "        # We normalize the features before computing the loss\n",
        "        normalized_features = features_map / (torch.sqrt(torch.mean(torch.square(features_map))) + 1e-5)\n",
        "\n",
        "        # Goal is to maximize the normalized channel's output\n",
        "        loss = -normalized_features[0, channel_idx].mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "  features_hook.close()\n",
        "  img = img.squeeze(0)\n",
        "  img = inv_normalize(img).cpu().detach().numpy()\n",
        "  img = np.transpose(img, (1,2,0))\n",
        "  img = np.clip(img, 0, 1)\n",
        "  plt.imshow(img)\n",
        "  plt.axis('off')\n",
        "  plt.tight_layout()\n",
        "\n",
        "#visualize layer one channel 2\n",
        "visualize_feature(model, 1, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmmxVqDsNi5H"
      },
      "source": [
        "Complete the following code to generate feature visaulizations of various filters of layer 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_MNrDnPMjjW"
      },
      "outputs": [],
      "source": [
        "layer = 1\n",
        "\n",
        "plt.figure(figsize=(20,25))\n",
        "for i, filter in enumerate(range(0, 3), 1):\n",
        "  ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ULTbythMjjW"
      },
      "source": [
        "Now let's try with layer 5, and 35 for example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6nAuMaHMjjW"
      },
      "outputs": [],
      "source": [
        "layer = 5\n",
        "\n",
        "plt.figure(figsize=(20,25))\n",
        "for i, filter in enumerate(range(0, 3), 1):\n",
        "  ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_nZzxARMjjX"
      },
      "outputs": [],
      "source": [
        "layer = 35\n",
        "\n",
        "plt.figure(figsize=(20,25))\n",
        "for i, filter in enumerate(range(0, 3), 1):\n",
        "  ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGDPtdnGMjjX"
      },
      "source": [
        "Obtaining nice visualizations as in the original publication requires some additional tricks.  \n",
        "For example, we can use a total variation loss to regularize the generated image.  \n",
        "Total variation loss is a regularization technique that encourages smoothness in our generated image by penalizing rapid changes in pixel intensity.  \n",
        "We can also use a Gaussian blur and Jitter to smooth the generated image.  \n",
        "Finally we can also clip the pixel values to the [0,1] range to keep the image in the same range as the natural images.  \n",
        "In fact, in the original publication, the authors used a more sophisticated regularization technique by optimizing the image in the Fourier space that we won't cover today.\n",
        "Feel free to read the publication and try it on other more sophisticated networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDjyw7g6OIdt"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms.functional import gaussian_blur\n",
        "\n",
        "def jitter(img, max_jitter=1):\n",
        "    jh, jw = random.randint(-max_jitter, max_jitter), random.randint(-max_jitter, max_jitter)\n",
        "    return torch.roll(img, shifts=(jh, jw), dims=(2, 3))\n",
        "\n",
        "def total_variation_loss(img):\n",
        "    tv_h = torch.mean(torch.abs(img[:, :, 1:, :] - img[:, :, :-1, :]))\n",
        "    tv_w = torch.mean(torch.abs(img[:, :, :, 1:] - img[:, :, :, :-1]))\n",
        "    return tv_h + tv_w\n",
        "\n",
        "def apply_gaussian_blur(img, kernel_size=3):\n",
        "    return gaussian_blur(img, (kernel_size, kernel_size))\n",
        "\n",
        "def visualize_feature(model, layer_idx, channel_idx, alpha=0.2, iterations=2048):\n",
        "    img = torch.rand((1, 3, 224, 224), requires_grad=True, device=\"cuda\")  # initialize a random image\n",
        "    optimizer = torch.optim.Adam([img], lr=0.05)\n",
        "    features_hook = FeaturesHook(model.features[layer_idx])  # hook the desired layer\n",
        "\n",
        "    for n in range(iterations):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Apply jitter\n",
        "        img_jittered = jitter(img)\n",
        "\n",
        "        # Apply Gaussian blur\n",
        "        img_blurred = apply_gaussian_blur(img_jittered)\n",
        "\n",
        "        model(img_blurred)  # forward pass with blurred image\n",
        "        features_map = features_hook.features\n",
        "\n",
        "        normalized_features = features_map / (torch.sqrt(torch.mean(torch.square(features_map))) + 1e-5)\n",
        "        feature_loss = -normalized_features[0, channel_idx].mean()\n",
        "\n",
        "        tv_loss = total_variation_loss(img)\n",
        "\n",
        "        # Combine feature loss and total variation loss\n",
        "        total_loss = feature_loss + alpha * tv_loss\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Pixel clipping\n",
        "        with torch.no_grad():\n",
        "            img.clamp_(-1, 1)\n",
        "\n",
        "    features_hook.close()\n",
        "    img = img.squeeze(0)\n",
        "    img = inv_normalize(img).cpu().detach().numpy()\n",
        "    img = np.transpose(img, (1, 2, 0))\n",
        "    img = np.clip(img, 0, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "visualize_feature(model, 35, 12)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-sKcUItMjjY"
      },
      "source": [
        "Use this code to generate feature visualizations of various filters of layer 35.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtT8xbiEN28L"
      },
      "outputs": [],
      "source": [
        "layer = 5\n",
        "\n",
        "plt.figure(figsize=(20,25))\n",
        "for i, filter in enumerate(range(0, 3), 1):\n",
        "  ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NGh_QLVN4oO"
      },
      "outputs": [],
      "source": [
        "layer = 35\n",
        "\n",
        "plt.figure(figsize=(20,25))\n",
        "for i, filter in enumerate(range(0, 3), 1):\n",
        "  ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyXB37EtMjjY"
      },
      "source": [
        "We can also use this technique to generate images that maximize the activation of an output unit corresponding to a specific class.   \n",
        "Here is an example for the ostrich class.  \n",
        "Do you recognize some patterns?  \n",
        "Try it on other classes. (list of classes [here](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shk8BD5pIGiY"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def visualize_output_neuron(model, output_neuron_idx, alpha=0.25, iterations=2048):\n",
        "    img = torch.rand((1, 3, 224, 224), requires_grad=True, device=\"cuda\")  # initialize a random image\n",
        "    optimizer = torch.optim.Adam([img], lr=0.1, weight_decay=1e-6)\n",
        "\n",
        "    for n in range(iterations):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        img_jittered = jitter(img)\n",
        "\n",
        "        # Apply Gaussian blur\n",
        "        img_blurred = apply_gaussian_blur(img_jittered)\n",
        "\n",
        "        output = model(img_blurred)  # forward pass with blurred image\n",
        "\n",
        "        # Maximize the activation of the chosen output neuron\n",
        "        neuron_activation = output[0, output_neuron_idx]\n",
        "        feature_loss = -neuron_activation\n",
        "\n",
        "        # Compute the total variation loss\n",
        "        tv_loss = total_variation_loss(img)\n",
        "\n",
        "        # Combine feature loss and total variation loss\n",
        "        total_loss = feature_loss + alpha * tv_loss\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Pixel clipping\n",
        "        with torch.no_grad():\n",
        "            img.clamp_(-1, 1)\n",
        "\n",
        "    img = img.squeeze(0)\n",
        "    img = inv_normalize(img).cpu().detach().numpy()\n",
        "    img = np.transpose(img, (1, 2, 0))\n",
        "    img = np.clip(img, 0, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "visualize_output_neuron(model, 9) # 'ostrich, Struthio camelus'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdD6NAc3QVk_"
      },
      "outputs": [],
      "source": [
        "visualize_output_neuron(model, 1, alpha=0.25) # jellyfish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8RoqZs8SOp8"
      },
      "outputs": [],
      "source": [
        "visualize_output_neuron(model, 385, alpha=0.25) # Indian elephant, Elephas maximus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2p1ljvRQ2Qa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}